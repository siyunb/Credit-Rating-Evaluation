---
title: "基于信用卡数据算法对比"
author: "中央财经大学    王思雨"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---
#一、台湾信贷研究目的与问题
##1.1 基于聚类算法
&emsp;&emsp;基于台湾信贷数据，建立各种聚类模型，根据聚类状况来判断在台湾信贷数据上不同聚类方法的表现。并且在方法论上，探讨每种聚类方法的特点，优势和算法原理，比较和分析每种聚类算法的优缺点和适用情况。
##1.2 基于分类算法
&emsp;&emsp;随机森林、bagging、adaboost 等方法都是基于决策树的组合方法，但是这些 方法和传统的统计思想有很大不同，如果针对具体数据集调整除合适模型来进行预 测是本文的核心，特别是针对信贷数据需要把握的特点即精确度的要求，对这三种 算法进行调试，并比较最终预测结果和这三种算法的优缺点。不论是 boosting 还 是 bagging 当中，当使用的多个分类器的类型都是一致的，如何针对数据集特征挖 掘不同算法和分类器的优势才能选择好的分类器。通过这篇报告，对这三种集成算 法可以有更清晰的了解。 
#二、台湾信贷数据集简介及变量说明
&emsp;&emsp;本此大数据机器学习实验从UCI机器学习数据集库中选取了经典台湾信贷数据集，并利用该数据来建立目前比较通用聚类模型和集成模型。该鲍鱼数据集是对30000个信贷用户记录个人信息状况、以往信用记录以及银行卡交易状况的描述信息。

&emsp;&emsp;信贷数据将这30000条信贷用户记录分为两类：下一月份违约的用“1”标识。以及下一月份未违约的用“0”标识。该数据集的属性信息以及变量描述如表1-1所示，表中给定的是变量名称，变量说明，变量类型和取值范围等。

$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$<font face="黑体" size=4>表1-1 属性变量说明表</font>

|变量名 | 变量说明 | 变量类型 | 取值范围|
|:-----:|:--------:|:--------:|:-------:|
|X1 | 给定额度 | 数值型 | 1万-100万|
|X2 | 性别 | 分类型 | 1：男，2：女|
|X3 | 教育水平 | 分类型 | 1:graduate;2:university;3:high school;4:others|
|X4 | 婚姻状况 | 分类型 | 1:married;2:single;3:others|
|X5 | 年龄 | 数值型 | 21-79|
|X6-X11 | 还款状态 | 数值型 | <0:按时还款，>0:超期相应月数还款|
|X12-X17 | 票据金额 | 数值型 | X12表示2015-09，以此类推|
|X18-X23 | 先前付款额 | 数值型 | X18表示2015-09，以此类推|
|Y | 是否违约 | 分类型 | 0：未违约，1：违约|

#三、聚类算法与分类算法工具包载入
&emsp;&emsp;载入与本次试验相关的数据处理包'reshape2','dplyr','pryr'等,绘图包'ggplot2','DiagrammeR'等，机器学习包'caret','xgboost'等以及性能测试包'pryr'等。由于本次试验需要对比各种机器学习算法的ROC曲线以及AUC值，所以进行了pROC的调用。
```{r message=FALSE, warning=FALSE, include=TRUE, paged.print=TRUE}
#首先需要进行xgboost包的安装，只需在R中运行install.packages(‘xgboost’)即可。
library(xgboost)      #导入极限梯度下降法包
library(caret)
library(ggplot2)      #绘图包
library(DiagrammeR)
library(reshape2)     #数据处理包
library(dplyr)
library(pryr) 
library(pROC)         #ROC曲线包
library(plyr)
library(showtext)     #使作图的字体更加丰富
library(RColorBrewer) #增加调色板
library(randomForest) #加载随机森林包
library(pryr)         #加载性能测试包
library(adabag)       #加载GBDT梯度提升树包
library(xgboost)      #加载XGBoost提升树包
library(DiagrammeR)
library(reshape2)
library(dbscan)
library(fpc)
library(sparcl)
```

#四、台湾信贷数据可视化分析
##4.1信贷数据预处理
```{r}
#数据预处理
#数据说明
taiwan<- read.csv("D:/bigdatahw/算法/作业/card1.csv",head=TRUE,sep=',') #读取台湾信用数据
taiwan$SEX<-as.factor(taiwan$SEX)         
taiwan$EDUCATION<-as.factor(taiwan$EDUCATION)
taiwan$MARRIAGE<-as.factor(taiwan$MARRIAGE)
names(taiwan)[25]="default"              #重命名因变量
taiwan$default<-as.factor(taiwan$default)

levels(taiwan$SEX)=list(M="1",F="2")     #进行重命名等级
levels(taiwan$EDUCATION)=list(others="0",graduate="1",university="2", highschool="3",others="4",others="5",others="6")
levels(taiwan$MARRIAGE)=list(married="1", single="2",others="3",others="0")
levels(taiwan$default)=list(T="1", F="0")
```

##4.2信贷数据描述性分析
###4.2.1教育状况与违约占比性别分面风玫瑰图
&emsp;&emsp;从图中可以看出，不管是男性还是女性，信贷用户中大学毕业生人数比较多，其次是研究生。其他学历人数较少，还可以看出男性的信贷违约人数要少于女性，说明女性更容易违约。在途中还可以看出随着学历的逐渐上升，违约状况也有所改观，说明教育水平会提升信用状况。
```{r message=FALSE, warning=FALSE}
label<-c("others","highschool","university","graduate")
taiwan$EDUCATION<- ordered(taiwan$EDUCATION, levels = label)
ggplot(taiwan,aes(x=default,fill=EDUCATION))+
  geom_bar()+coord_polar(theta = 'x')+
  scale_fill_brewer(palette='Spectral')+facet_wrap(~SEX)+theme_bw()+ 
  labs(x="违约状况",y="频数",fill="学历状况",title='分面风玫瑰图')+
  scale_x_discrete()+coord_polar(theta="x")+
  theme(plot.title = element_text(hjust = 
        0.5,family="myFont",size=18,color="gold3"),
        panel.background=element_rect(fill='aliceblue')) 
```

###4.2.2违约与年龄分布直方图
&emsp;&emsp;根据所受教育水平的不同，违约状况的的年龄分布也有所不同，在学历为其他的用户中违约的年龄分布比较均匀，在高中毕业的水平下，总体的年龄分布比较均匀，违约人的年龄分布也是比较均匀的，在大学的教育水平下，年龄分布比较不均匀，整体成右偏分布，而违约人的年龄分布峰较低，比较平缓。在研究生阶段的的年龄分布峰较高，成尖峰分布，且有右偏拖尾。整体的违约比率较低，分布比较平缓。
```{r message=FALSE, warning=FALSE}
label<-c("F","T")
taiwan$default<- ordered(taiwan$default, levels = label)
p<-ggplot(taiwan,aes(x=AGE,fill=default)) 
p+geom_histogram(position="identity",alpha=0.5)+
  ggtitle('各学历违约状况与年龄分布直方图')+
  theme(plot.title = element_text(hjust = 
        0.5,family="myFont",size=18,color="red"),
        panel.background=element_rect(fill='aliceblue')) + 
  xlab("年龄") +ylab("频数")+
  facet_wrap(~EDUCATION)
```

###4.2.3银行给定额度与违约箱线图
&emsp;&emsp;如图所示，白色的点代表着中位数，而象限图中的白线则代表着均值。对于给定的信用额度与违约的关系上来看，可以看出的关系是，对于未来不违约的人通常银行给予的信用额度是比较高的，说明银行在当时信用分配筛选时，是下过一番功夫的。因此会导致未来违约的人的信用额度偏低。
```{r message=FALSE, warning=FALSE}
ggplot(taiwan,aes(x=default,y=LIMIT_BAL,fill=default))+
  geom_boxplot(outlier.size=1.5, outlier.shape=15,notch=TRUE,alpha=.35)+
  stat_summary (fun.y="mean",geom="point",shape=23,size=2,fill="white")+
  xlab("违约与否") + ylab("给定的信用额度")+
  ggtitle('给定信用额度与违约箱线图')+ylim(0,550000)+
  theme(plot.title = element_text(hjust = 0.5,  
        family="myFont",size=18,color="red"), 
        panel.background=element_rect(fill='aliceblue',color='black')) 
```

###4.2.4婚姻状况与违约之间关系
&emsp;&emsp;在图中可以看出婚姻状况与违约之间的关系，数量关系没有描述统计的必要，主要进行百分比之间的转化，在分面饼状图中可以清楚的看出，婚姻状况对违约并没有很大的影响，主要的影响是比较细微的，单身的违约概率是比较小的，大约占到总体的1/5左右，而已婚和其他的状况违约概率是几乎一样的，都占各自总体的1/4左右。
```{r message=FALSE, warning=FALSE}
ggplot(taiwan,aes(x=factor(1),fill=default))+
  geom_bar(aes(fill=default),position="fill")+
  coord_polar(theta="y")+
  ggtitle('婚姻状况与违约之间关系')+
  theme(plot.title = element_text(hjust = 
       0.5,family="myFont",size=18,color="black"),      
       panel.background=element_rect(fill='aliceblue',color='black'))+
       facet_wrap(~MARRIAGE) 
```

###4.2.5教育与违约关系
&emsp;&emsp;在教育水平与违约状况环形图中，也可以看到，违约占比较高的教育水平为高中阶段，其次是大学，然后是研究生，相比之下，其他的教育水平违约占比较低，因此我们也可以大胆的推断other是一种更高的教育水平。有可能是博士阶段。各个学历水平的违约率都没超过1/4，other更是小于1/10。
```{r message=FALSE, warning=FALSE}
ggplot(taiwan, aes(EDUCATION))+geom_bar(aes(fill=default),position="fill")+
  coord_polar(theta = "y")+ggtitle('教育水平与违约之间关系')+
  theme(plot.title = element_text(hjust = 
        0.5,family="myFont",size=18,color="black"),      
        panel.background=element_rect(fill='aliceblue',color='black'))
```

###4.2.6年龄与给定额度关系
&emsp;&emsp;我们筛选的是具有违约状况的信用记录来进行分析。由于点的数量太多，因此绘制了分箱密度估计图，横坐标是违约用户的年龄，纵坐标是给定额度的对数值。来分析对于违约的用户，什么样的年龄和给定额度下违约的人数较多，可以在图中看到，红色区域对应着25岁给定额度为10的人数违约较多，其次还有25岁给定额度为11左右的人。因此银行在分配额度是应该注意这一点，利用密度图来规避风险，尽量少分配给25年龄段更多的信用。还可以看出给定额度与年龄的关系并不大，年龄小一样可以获得较高额度。
```{r message=FALSE, warning=FALSE}
taiwan<-taiwan[which(taiwan$default=='T'),]
p=ggplot(taiwan,aes(x=AGE,y=log(LIMIT_BAL)))
#默认等高线图进行分箱化处理
p+geom_point(alpha=0.2)+stat_bin2d()+
  scale_fill_gradient(low="lightblue",high="red")+stat_density2d()+
  theme(plot.title = element_text(hjust = 
        0.5,family="myFont",size=18,color="slateblue2"),
        panel.background=element_rect(fill='papayawhip'))+
  labs(x='年龄',y='log(给定额度)',title='年龄与给定额度密度关系')
```

#五、基于台湾信贷数据的聚类方法比较与分析
##5.1 数据预处理
```{r message=FALSE, warning=FALSE}
setwd('D:/bigdatahw/算法/作业')   #设置工作路径
taiwan=read.csv("card1.csv")
names(taiwan)[25]="default"       #重命名因变量
#特征变量性别、年龄、教育程度以及下月是否违约转换为因子变量
taiwan$default<- as.factor(taiwan$default)
taiwan$SEX <- as.factor(taiwan$SEX)
taiwan$EDUCATION <- as.factor(taiwan$EDUCATION)
taiwan$MARRIAGE <- as.factor(taiwan$MARRIAGE)
summary(taiwan)
## 定义标准化函数，将原始数据压缩到0-1之间，同一量纲
myfun=function(x)
{
  m1=max(x)
  m2=min(x)
  y=(x-m1)/(m2-m1)
  return(y)
}
## 抽取部分样本作为建模数据，并进行标准化处理
set.seed(123)
s=sample(1:dim(taiwan)[1],500)
taiwan_num=apply(taiwan[s,c(2,6:24)],2,myfun)    #聚类数值数据
taiwan_char_num=cbind(taiwan_num,taiwan[s,3:5])  #聚类数值+分类数据
```

##5.2AGNES算法层次聚类

```{r message=FALSE, warning=FALSE}
## 由于聚类分析是无监督分类，无法知道聚类结果到底是属于哪一类。
## 所以要定义函数变换混淆矩阵，使得对角线上的元素最大，然后输
## 出混淆矩阵，以便确定聚类结果分别属于哪一个类。定义函数找出
## 对角线上数量最大的矩阵
myfun1=function(a,b)
{
  table<-table(a, b)  
  Accuracy1=table[1,1]+table[2,2]
  Accuracy2=table[1,2]+table[2,1]
  if(Accuracy1>Accuracy2){
    confusion<-table(a,b)
    rownames(confusion)<-c('未违约','违约')
    colnames(confusion)<-c('未违约','违约')
    return(confusion)}
  else{ label<-c("2","1")
        b<- ordered(b, levels = label)
        confusion<-table(a,b)
        rownames(confusion)<-c('未违约','违约')
        colnames(confusion)<-c('未违约','违约')
        return(confusion)}
}
d<-dist(taiwan_num)               #计算样本之间的欧式距离
hc<-hclust(d, method="complete")  #根据距离进行聚类,类间距离定义为全连接
plot(hc)                          #显示聚类结果图
hccut<-cutree(hc,k=2)             #将类别数定为2
myfun1(taiwan[s,]$default, hccut) #需要变0-1
```

##5.3Kmeans聚类
```{r message=FALSE, warning=FALSE}
kc<-kmeans(taiwan_num,2)                 #k均值聚类将样本聚成2类
myfun1(taiwan[s,]$default, kc$cluster)   
#显示混淆矩阵，可以调整以下顺序就会不同了，遍历一下
```

##5.4DBSCAN基于密度聚类
```{r message=FALSE, warning=FALSE}
##确定最优eps
kNNdistplot(taiwan_num,k=2)
abline(h=0.3)
#可达距离eps设置为0.45，最小可达数为50，也就是说核心点的最小可达点最少要50个
ds=dbscan(taiwan_num,eps=0.1,MinPts=500,
          scale=TRUE,showplot=TRUE,method="raw")     #显示聚类图谱图
ds                                                   #显示聚类结果
table(taiwan[s,]$default, ds$cluster)

#optics算法，载入dbscan层次聚类包，采用的算法为optics,eps可达距离为1。
#ε邻域大小的上限。限制邻域大小可以提高性能，而且只要设置得不太低，就
#不会对排序产生什么影响。稀疏K均值聚类
opt<-optics(d, eps=1, minPts=4) 
plot(opt)                       #可达距离图
```

##5.5稀疏聚类
###5.5.1Kmeans稀疏聚类
```{r message=FALSE, warning=FALSE}
perm <- KMeansSparseCluster.permute(taiwan_num,K=2,wbounds=seq(1.5,7.5,0.5),nperms=3)#选择最优的tunings
print(perm)
km<- KMeansSparseCluster(taiwan_num,K=2,wbounds=perm$bestw)#在最优的tuning下求解最优权重，并聚类
print(km)
barplot(km[[1]][[1]])#可以看到哪些特征非零
myfun1(taiwan[s,]$default, km[[1]][[2]])
```

###5.5.2层次稀疏聚类

```{r message=FALSE, warning=FALSE}
#稀疏层次聚类的调优参数选择
perm.out <- HierarchicalSparseCluster.permute(as.matrix(taiwan_num), wbounds=c(1.5,2:6),nperms=10)
print(perm.out)
plot(perm.out)

# 执行稀疏层次聚类
sparsehc <- HierarchicalSparseCluster(dists=perm.out$dists,
                                      wbound=perm.out$bestw, method="complete")
par(mfrow=c(1,2))
plot(sparsehc)
plot(sparsehc$hc)
print(sparsehc)
cutree(sparsehc$hc,2)
myfun1(taiwan[s,]$default,cutree(sparsehc$hc,2))
# 使用类标签知识来比较真实类
#获得聚类的标签
par(mfrow=c(1,1))
y = cutree(hc, 3)
ColorDendrogram(hc,y=y,main="My Simulated Data",branchlength=.007)
#现在，如果我们想看看数据是否包含了一个*二次聚类，
#在计算第一个得到的数据之后。我们寻找互补稀疏聚类：
# 重做分析，但这次使用“绝对值”不同：
perm.out <- HierarchicalSparseCluster.permute(as.matrix(taiwan_num),
                                              wbounds=c(1.5,2:6),
                                              nperms=5, 
                                              dissimilarity="absolute.value")
print(perm.out)
plot(perm.out)
# 执行稀疏层次聚类
sparsehc <- HierarchicalSparseCluster(dists=perm.out$dists, wbound=perm.out$bestw, method="complete", dissimilarity="absolute.value")
par(mfrow=c(1,2))
plot(sparsehc)
```

##5.6双向聚类
###5.6.1数据预处理
```{r message=FALSE, warning=FALSE}
##双向聚类
##数据预处理,将分类变量处理成独热编码
library(caret)
taiwan=read.csv("card1.csv")
ohe_feats = c('SEX', 'EDUCATION', 'MARRIAGE')
taiwan$SEX<-as.factor(taiwan$SEX)
taiwan$EDUCATION<-as.factor(taiwan$EDUCATION)
taiwan$MARRIAGE<-as.factor(taiwan$MARRIAGE)
dummies <- dummyVars(~ SEX +  EDUCATION + MARRIAGE, data = taiwan)
df_all_ohe <- as.data.frame(predict(dummies, newdata = taiwan))
taiwan_bi <- cbind(df_all_ohe,taiwan_num)
```

###5.6.2双向聚类分析
```{r message=FALSE, warning=FALSE, error=FALSE}
library(biclust)
taiwan_bi=as.matrix(taiwan_bi[s,])
heatmap(taiwan_bi)
bidata=binarize(taiwan_bi)                 #处理成二分数据
bic<-biclust(bidata,method=BCBimax(),minr=400, minc=3)
bic #以BIMAX方法进行聚类
heatmapBC(x = bidata,
          bicResult = bic,main="BIMAX算法双向聚类热力图")

bic1<-biclust(bidata,method=BCCC(), number=2)
bic1 #以CC方法进行聚类
##heatmapBC(bidata,bic1,main="CC算法双向聚类热力图")  #做出热力图
```
![](D:\bigdatahw\算法\作业\Rplot.jpeg)

##5.7聚类算法对比
&emsp;&emsp;在第五章中，基于台湾信贷数据进行了不同方式的聚类。不同的聚类方式对类别间的差异程度的衡量有所不同，层次聚类和k均值聚类是根据点与点之间的距离进行聚类的，衡量距离的定义有很多，本章中采用的都是欧氏距离。而Dbscan聚类是基于密度的，将密度大的区域作为一个类，而将密度小的区域作为类与类的边界。

&emsp;&emsp;在基于距离的聚类中，AGNES层次聚类是一种自下而上的一种层次能聚方法，不断地将点通过相似的距离进行融合，而k均值聚类则是通过给定的k个类，不断迭代，使规划目标类间距离最小，从而找到核心点，确定类别。

&emsp;&emsp;而基于密度聚类的Dbscan聚类相较于基于距离聚类的优势就是可以做出任何形式的类，而不需要提前给出类的个数K的个数。缺点就单纯的依靠点与点之间的密度进行距离，衡量的是某一块区域之内的点的个数，缺乏现实意义，聚类比较盲目，也可以再基于台湾的信贷数据上看到，的确其准确度较低。

&emsp;&emsp;对于层次和k均值聚类，使用的都是数据的全部特征，这样做不但会增加计算量是计算效率降低，还会使得聚类效果不明显，忽略一些重要的变量，稀疏聚类的优势就是通过增加W权重，并对权重进行范数为1的限制，来对每一个特征指标在聚类中的表现进行限制。会突出一别变量的作用，在特征变量较多的状况的基因数据下，表现较好。起到数据压缩的效果。

&emsp;&emsp;双向聚类则是对观测和指标都进行聚类，以这种方式来反映一些变量在某些指标下的相似状况，而非全部特征下，因为能在全部指标下某些观测是不具有相似性的，所以更加具有实际意义。且尚香聚类的BIMAX在处理系数矩阵时具有独特的优势，在储存空间和运算时间上都较少。CC算法对BIMX的改进再在于，它弥补了在观测和指标较多是进行单列，但行的单节点删除的BIMAX法的运行效率较低，采用了大规模的多节点删除算法和节点添加算法，在大数据量下的表现会更加好。运行时间会进一步缩短。
&emsp;&emsp;最后，基于台湾信贷数据的聚类运算结果表如下表所示：

$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$<font face="黑体" size=4>表1-2 信贷数据聚类算法比较表</font>

| | 准确率 | 运行时间（s） | 
|:-----:|:--------:|:--------:|
|层次聚类 | 76.2% | 0.25 |
|k-means聚类 | 66.2% | 0.85 | 
|dbcan密度聚类 | 76.4% | 0.67 | 
|k-means稀疏聚类 | 58.2% | 0.94 |  
|层次稀疏聚类 | 74.6% | 0.45 |

&emsp;&emsp;从对比表来看，基于台湾信贷数据上来看，在准确率上密度聚类的效果最好，而从运行时间上来看，层次聚类由于计算复杂小，所以耗费时间最短。

#六、基于台湾信贷数据的分类算法的比较与分析
##6.1 随机森林算法
随机森林是一种很灵活实用的方法，它有如下几个特点： 
   
 - 在当前所有算法中，具有极好的准确率  
 - 能够有效地运行在大数据集上 
 - 能够处理具有高维特征的输入样本，而且不需要降维 
 - 能够评估各个特征在分类问题上的重要性 
 - 在生成过程中，能够获取到内部生成误差的一种无偏估计 
 - 对于缺省值问题也能够获得很好得结果 
 
###6.1.1 随机森林数据预处理
```{r message=FALSE, warning=FALSE}
#数据预处理
alldata <- read.csv("D:/bigdatahw/算法/作业/card1.csv",head=TRUE,sep=',')
alldata$default.payment.next.month <- as.factor(alldata$default.payment.next.month)
alldata$SEX <- as.factor(alldata$SEX)
alldata$EDUCATION <- as.factor(alldata$EDUCATION)
alldata$MARRIAGE <- as.factor(alldata$MARRIAGE)
alldata$PAY_0 <- as.factor(alldata$PAY_0)
alldata$PAY_2 <- as.factor(alldata$PAY_2)
alldata$PAY_3 <- as.factor(alldata$PAY_3)
alldata$PAY_4 <- as.factor(alldata$PAY_4)
alldata$PAY_5 <- as.factor(alldata$PAY_5)
alldata$PAY_6 <- as.factor(alldata$PAY_6)
#切分训练集和测试集
n <- sample(1:nrow(alldata),0.8*nrow(alldata))
train <- alldata[n,-1]
test <- alldata[-n,-1]
```

###6.1.2 随机森林实证检验
```{r message=FALSE, warning=FALSE}
rf <- randomForest(default.payment.next.month~.,data = train,
                   mtry=9,ntree=200,importance = T)
#查看训练集错误率随树的数量变化情况
plot(rf)
#查看训练集平均错误率
mean(rf$err.rate)
#训练集的混淆矩阵
rf$confusion
#变量重要性
imp <- importance(x=rf)
imp
varImpPlot(rf)
#运行时间和消耗内存
#timecost <- system.time(randomForest(default.payment.next.month~.,
                                     #data = train,
                                     #mtry=9,ntree=500,
                                     #importance = T))
#print(timecost)
#定义评价函数
index2=function(table) {
  Accuracy=table[1,1]+table[2,2]
  #真阳性率+真阴性率=预测准确率
  precision=table[2,2]/sum(table[,2]) 
  #预测阳性率=精确度
  recall=table[2,2]/sum(table[2,]) 
  #第二类错误，灵敏度 =召回率
  F_measure=2*precision*recall/(precision+recall)
  #对前两者进行加权，综合结果
  results=data.frame(Accuracy=Accuracy,
                     recall=recall,
                     precision=precision,
                     F_measure=F_measure)
  return(results)
}
#测试集进行检验
pred1 <- predict(rf,test[,-25])
#测试集混淆矩阵
table(pred1,test$default.payment.next.month)
#测试集评价
rf.pred= predict(rf,newdata=test[,-25]) 
rf.real=test$default.payment.next.month
table_RF=table(rf.real,rf.pred)/nrow(test)
a=index2(table_RF)
print(a)
#ROC曲线
library(ROCR)
re.pred= predict(rf,newdata=test[,-25],type = "prob")
rocr = prediction(as.vector(re.pred[,2]),
                  test$default.payment.next.month)
AUC = performance(rocr,"auc")
print(AUC@y.values[[1]])
ROC = performance(rocr,"tpr","fpr")
plot(ROC,main="ROC plot")
text(0.5,0.5,paste("AUC = ",
                   format(AUC@y.values[[1]],
                          digits=5,
                          scientific=FALSE)))
```

##6.2 Adaboost集成算法
&emsp;&emsp;AdaBoost 采取加权多数表决的方法。 具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小 分类误差率大的弱分类器的权值，使其在表决中起较小的作用。 

###6.2.1 Adaboost数据预处理
```{r message=FALSE, warning=FALSE}
credit <- read.csv("D:/bigdatahw/算法/作业/card1.csv",head=TRUE,sep=',')
credit<-credit[,-1] #去除id行
names(credit)[24] <- 'label'
credit$label <- as.factor(credit$label)
credit$SEX <- as.factor(credit$SEX)
credit$EDUCATION <- as.factor(credit$EDUCATION)
credit$MARRIAGE <- as.factor(credit$MARRIAGE)
credit$PAY_0 <- as.factor(credit$PAY_0)
credit$PAY_2 <- as.factor(credit$PAY_2)
credit$PAY_3 <- as.factor(credit$PAY_3)
credit$PAY_4 <- as.factor(credit$PAY_4)
credit$PAY_5 <- as.factor(credit$PAY_5)
credit$PAY_6 <- as.factor(credit$PAY_6)

#按标签比例各20%筛选训练集与测试集
idx0 <- which(credit$label == 1)
idx1 <- which(credit$label == 0)

cls0 <- sample(idx0,round(0.2 * length(idx0)))
cls1 <- sample(idx1,round(0.2 * length(idx1)))

tst <- credit[c(cls0,cls1),]
trn <- credit[-c(cls0,cls1),]

#查看各数据矩阵维度
dim(credit)
dim(tst)
dim(trn)
```

###6.2.2 Adaboost实证检验
```{r message=FALSE, warning=FALSE}
now <- Sys.time()
mem_change(model.AdaBoost <- boosting(label~.,data = trn))
Sys.time() - now
model.pred <- predict(model.AdaBoost,newdata = tst,type='class')
model.pred$confusion      #查看混淆矩阵

cal=function(table) {
  Accuracy=table[1,1]+table[2,2] 
  #真阳性率+真阴性率=预测准确率
  precision=table[2,2]/sum(table[,2]) 
  #预测阳性率=精确度
  recall=table[2,2]/sum(table[2,]) 
  #第二类错误，灵敏度 =召回率
  F_measure=2*precision*recall/(precision+recall)
  #对前两者进行加权，综合结果
  results=data.frame(Accuracy=Accuracy,recall=recall,precision=precision,F_measure=F_measure)
  return(results)
}
cal(model.pred$confusion / 6000)
```

##6.3 XGBoost集成算法
 XGBoost 算法总结起来大致其有三个优点：高效、准确度、模型的交互性：

 - 正则化：标准 GBDT 提升树算法的实现没有像 XGBoost 这样的正则化  步骤。正则化用于控制模型的复杂度，对减少过拟合也是有帮助的。XGBoost  也正是以“正则化提升”技术而闻名。 
 
 - 并行处理：XGBoost 可以实现并行处理，相比 GBM 有了速度的飞跃。 不过，需要注意 XGBoost 的并行不是 tree 粒度的并行，XGBoost 也是一次迭 代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代 的预测值）。XGBoost 的并行是在特征粒度上的。决策树的学习最耗时的一个 步骤就是对特征的值进行排序（因为要确定最佳分割点）。因此 XGBoost 在 R 重定义了一个自己数据矩阵类 DMatrix。XGBoost 在训练之前，预先对数据进 行了排序，然后保存为 block 结构，后面的迭代中重复利用索引地使用这个结 构，获得每个节点的梯度，大大减小计算量。这个 block 结构也使得并行成为 了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的 那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 
 
  - 高度灵活性：XGBoost 允许用户定义自定义优化目标和评价标准，它 对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。 
  
  - 缺失值处理：XGBoost 内置处理缺失值的规则。 用户需要提供一个 和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取 值。XGBoost 在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来 遇到缺失值时的处理方法。 
  
  - 剪枝：当分裂时遇到一个负损失时，传统 GBDT 会停止分裂。因此传 统 GBDT 实际上是一个贪心算法。XGBoost 会一直分裂到指定的最大深度 (max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除 这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10） 的时候，就显现出来了。GBM 会在-2 处停下来，因为它遇到了一个负值。但是 XGBoost 会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这 两个分裂。
  
   - 内置交叉验证：XGBoost 允许在每一轮 boosting 迭代中使用交叉验 证。因此，可以方便地获得最优 boosting 迭代次数。而传统的 GBDT 使用网格 搜索，只能检测有限个值
   
###6.3.1 XGBoost数据预处理
```{r message=FALSE, warning=FALSE}
#读取训练数据集与测试数据集
taiwan<- read.csv("D:/bigdatahw/算法/作业/card1.csv",head=TRUE,sep=',') #读取台湾信用数据
set.seed(1234)      #设置随机种子，可以重复实验结果
tra=sample(1:nrow(taiwan), round(0.8* nrow(taiwan)))  #抽取80%,24000个数据作为训练集
train=taiwan[tra,-1]
test=taiwan[-tra,-1]
#独热编码分类特征
ohe_feats = c('SEX', 'EDUCATION', 'MARRIAGE')
train$SEX<-as.factor(train$SEX)
train$EDUCATION<-as.factor(train$EDUCATION)
train$MARRIAGE<-as.factor(train$MARRIAGE)
dummies <- dummyVars(~ SEX +  EDUCATION + MARRIAGE, data = train)
df_all_ohe <- as.data.frame(predict(dummies, newdata = train))
train <- cbind(df_all_ohe,train[,-c(2,3,4)])

ohe_feats = c('SEX', 'EDUCATION', 'MARRIAGE')
test$SEX<-as.factor(test$SEX)
test$EDUCATION<-as.factor(test$EDUCATION)
test$MARRIAGE<-as.factor(test$MARRIAGE)
dummies <- dummyVars(~ SEX +  EDUCATION + MARRIAGE, data = test)
df_all_ohe <- as.data.frame(predict(dummies, newdata = test))
test<- cbind(df_all_ohe,test[,-c(2,3,4)])

#切分训练特征与回归特征
x<-train[,1:33]
y<-train[,34]
x<-apply(x,2,as.numeric)        #将x的列变量转化为数字型，xgboost对数据类型要求严格
y<-as.numeric(y)             #将y也转化为数字型变量
```

###6.3.2 XGBoost实证检验
```{r message=FALSE, warning=FALSE}
# xgboost训练过程
#也可以利用函数xgb.cv对数据进行交叉验证分析，利用交叉验证确定均方误差,
#利用交叉检验确定最佳迭代次数，利用10折交叉验证
cv.res<-xgb.cv(data=x,label=y,max.depth=2,eta=1,nround=15,objective='binary:logistic',nfold=10)
cv.res <- as.data.frame(cv.res$evaluation_log)
cv.res<-melt(cv.res[,c(1,2,4)],id = c("iter"),     
       variable.name = "type", 
       value.name = "cases",
       na.rm = TRUE)                #控制两列变量在新表中不动新生成的变量用case标识
ggplot(data=cv.res, aes(x=iter, y=cases, group=type, colour=type)) +
  geom_line(size=1) +
  geom_point() +
  xlab("决策树棵数") + ylab("均方误差")+
  ggtitle('交叉检验确定最优棵数')+
  theme(plot.title = element_text(hjust =0.5,family="myFont",size=20,color="red"), 
        panel.background=element_rect(fill='aliceblue',color='black'),panel.grid.minor = element_blank())
#可以从10折交叉验证结果来看，当决策树数量（也就是迭代次数）
#到达8次后测试误差率开始上升说明XGBoost模型开始过于复杂，
#甚至出现了一点过拟合迹象因此选择决策树棵数也就是迭代次数为8次。

#定义XGBoost模型测试集结果评价函数，
#包含Accuracy，recall，precision，F_measure四个评价指标
#Accuracy总正确率，recall后验准确率，precision先验准确率，F_measure指数
index2=function(table) {
  Accuracy=table[1,1]+table[2,2]
  precision=table[2,2]/sum(table[,2])
  recall=table[2,2]/sum(table[2,])
  F_measure=2*precision*recall/(precision+recall)#计算Recall，Precision和F-measure
  results=data.frame(Accuracy=Accuracy,recall=recall,precision=precision,F_measure=F_measure)
  return(results)
}

timestart<-Sys.time();
bst<-xgboost(data=x,label=y,max.depth=2,eta=1,nround=8,objective='binary:logistic')
#在此处，根据交叉验证结果我们选取最大深度为2，学习速率为1，
#决策树棵树为10,也就是迭代次数为10，目标函数是基于二分类问
#题的logistic损失进行模型建立。
timeend<-Sys.time()
runningtime<-timeend-timestart
print(runningtime)             #运行时间
#利用predict函数进行预测
test<-apply(test,2,as.numeric) 
pred<-predict(bst,test[,-34])
pred1 = ifelse(pred>0.5,1,0)   #转化为分类变量0-1
true<-as.factor(test[,34])
table_XG=table(true,pred1)/nrow(test)	 #混淆数量矩阵
table_XG                       #从混淆矩阵中看出并不好  
a=index2(table_XG)             #预测准确率为0.771%,速度很快
a
#绘制ROC曲线
xgb_lr.train.modelroc <- roc(test[,34], pred)  
plot(xgb_lr.train.modelroc,print.auc=TRUE,auc.polygon=TRUE,grid=c(0.1,0.2),  
     grid.col=c("green","red"),max.auc.polygon=TRUE,auc.polygon.col="skyblue",print.thres=TRUE) 

#XGBoost变量重要性
model <- xgb.dump(bst, with.stats = T)
model[1:10]
# 获得特征的真实名称
names <- dimnames(train)[[2]]
# 计算特征重要性矩阵
importance_matrix <- xgb.importance(names, model = bst)
# 制图
xgb.ggplot.importance(importance_matrix[1:10,], rel_to_first = TRUE)+ylab('Gain')
xgb.plot.tree(model = bst,n_first_tree = 1,plot_width = 600,plot_height = 600)
#查看消耗内存
mem_change(xgboost(data=x,label=y,max.depth=2,eta=1,nround=12,objective='binary:logistic'))
```

###6.3.3 XGBoost模型优化
```{r}
###利用XGBoost构造新的特征提高分类效率
train <- data.matrix(train)
test <- data.matrix(test)
new.features.train <- xgb.create.features(model = bst, train[,-34])  
# 生成xgboost构造的新特征组合，训练集  
new.features.test <- xgb.create.features(model = bst, test[,-34])    
# 生成xgboost构造的新特征组合，测试集  

newdtrain <- as.data.frame(as.matrix(new.features.train))        
# 将训练集的特征组合转化为dataframe格式  
newdtest <- as.data.frame(as.matrix(new.features.test))         
# 将测试集的特征组合转化为dataframe格式  

newtraindata <- cbind(newdtrain,y=train[,34])             
# 将训练集的自变量和因变量合并  
newtestdata <- cbind(newdtest,y=test[,34])               
# 将测试集的自变量和因变量合并 

# 对训练集进行预测  
x1<-newtraindata[,1:58]
y1<-newtraindata[,59]
x1<-apply(x1,2,as.numeric)        
#将x的列变量转化为数字型，xgboost对数据类型要求严格
y1<-as.numeric(y1)                     #将y也转化为数字型变量
cv.res<-xgb.cv(data=x1,label=y1,max.depth=2,eta=1,nround=15,objective='binary:logistic',nfold=10) 
#十折交叉检验确定迭代次数
bst<-xgboost(data=x1,label=y1,max.depth=2,eta=1,nround=20,objective='binary:logistic')
newtestdata<-apply(newtestdata,2,as.numeric) 
pred<-predict(bst,newtestdata[,-59])
pred1 = ifelse(pred>0.5,1,0)           #转化为分类变量0-1
true<-as.factor(newtestdata[,59])
table_XG=table(true,pred1)/nrow(test)	 #混淆数量矩阵
table_XG    
a=index2(table_XG)                     #预测准确率为0.771%,速度很快
a
#绘制ROC曲线
xgb_lr.train.modelroc <- roc(newtestdata[,59], pred)  
plot(xgb_lr.train.modelroc,print.auc=TRUE,auc.polygon=TRUE,grid=c(0.1,0.2),  
     grid.col=c("green","red"),max.auc.polygon=TRUE,auc.polygon.col="skyblue",print.thres=TRUE) 
```

##6.4分类算法对比
&emsp;&emsp;先不考虑信贷的实际问题，单纯比较算法性能来讲，集成算法的对比表如下表所示，可以看出单从准确率上看，算法之间差距并不是很大，说明在集成算法上可 能都已经逼近了数据本身应有的特征，无法有大的改进，但值得注意的是，随机森林和 Adaboost 都需要进行多棵决策树的生成，迭代次数较多，成千上百次，而XGBoost只需要进行少量的迭代次数，在本次试验中仅需要8次。而且运行时间上XGBoost的多线程并行和基于其block的索引数据结构运行，也发挥了极大作用。比Adaboost缩短时间达到了近400倍，而比随机森林缩短时间近200倍。在本次信贷的数据分析中，XGBoost更有调节阈值后可以更加符合银行的商业要求，更加准确的识别出违约人。

$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$<font face="黑体" size=4>表1-2 算法比较表</font>

| | 随机森林 | Adaboost | XGBoost | 改进XGBoost|
|:-----:|:--------:|:--------:|:-------:|
|Accuracy | 0.81 | 0.82 | 0.82| 0.83 |
|recall | 0.37 | 0.34 | 0.36| 0.36 |
|precision | 0.62 | 0.67 | 0.68| 0.68 |
|F_measure | 0.46 | 0.45 | 0.48| 0.49 |
|运行时间(s)  | 44.46 | 85.42 | 0.21| 0.23 |
